{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da596a0b",
   "metadata": {},
   "source": [
    "<h1>An Introduction to HuggingFace Models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb47fc",
   "metadata": {},
   "source": [
    "1. Pipeline\n",
    "2. AutoModel\n",
    "3. Pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fca328",
   "metadata": {},
   "source": [
    "<h4>Pipeline</h4>\n",
    "\n",
    "Pipelines are ideal for quick application of pre-trained models to specific tasks without extensive coding.<br>\n",
    "They handle input preprocessing, model execution, and output postprocessing.<br>\n",
    "Users have limited control over model architectures, hyperparameters, and training procedures, which may restrict flexibility for specific use cases, as They offer limited customization options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f86e7",
   "metadata": {},
   "source": [
    "<h4>AutoModel</h4>\n",
    "\n",
    "Users can fine-tune or customize various aspects of the model, such as architecture, tokenizer, optimizer, learning rate scheduler, and training procedure.<br>\n",
    "This level of customization allows for fine-grained control over the entire NLP pipeline, from data preprocessing to model training and evaluation.<br>\n",
    "You have less direct control over the model architecture itself.<br>\n",
    "Fine-tuning AutoModels is less straightforward compared to working directly with the pre-trained model.<br>\n",
    "You might need to dig deeper into the underlying code to access fine-tuning capabilities. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8119f8d",
   "metadata": {},
   "source": [
    "<h4>Pre-trained Model</h4>\n",
    "\n",
    "Pretrained models offer the most flexibility and control over model configuration and training process.<br>\n",
    "Users can fine-tune pretrained models, adjust hyperparameters, modify architectures, and integrate with custom components to tailor the model to their specific needs.<br>\n",
    "Pretrained models are suitable for advanced users or researchers who require full control over every aspect of the NLP pipeline.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d594e03",
   "metadata": {},
   "source": [
    "<h2>Pipeline</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1548c",
   "metadata": {},
   "source": [
    "1. Text Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e830be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text classification pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Perform classification on a single text\n",
    "result = classifier(\"I love this product!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7a156",
   "metadata": {},
   "source": [
    "2. Named Entity Recognition (NER):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NER pipeline\n",
    "ner = pipeline(\"ner\")\n",
    "\n",
    "# Perform NER on a single text\n",
    "result = ner(\"Apple is a company founded by Steve Jobs.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992e1ac",
   "metadata": {},
   "source": [
    "3. Text Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text generation pipeline\n",
    "generator = pipeline(\"text-generation\")\n",
    "\n",
    "# Generate text based on a prompt\n",
    "result = generator(\"Once upon a time\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2bf4da",
   "metadata": {},
   "source": [
    "4. Question Answering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4566f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a question answering pipeline\n",
    "qa = pipeline(\"question-answering\")\n",
    "\n",
    "# Provide context and question\n",
    "context = \"The Hugging Face Transformers library was developed by Hugging Face.\"\n",
    "question = \"Who developed the Hugging Face Transformers library?\"\n",
    "result = qa(question=question, context=context)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f1203",
   "metadata": {},
   "source": [
    "5. Summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6061c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Summarize a piece of text\n",
    "text = \"The Hugging Face Transformers library provides state-of-the-art natural language processing models.\"\n",
    "result = summarizer(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb07c48",
   "metadata": {},
   "source": [
    "<h2>AutoModel</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefc7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f345005",
   "metadata": {},
   "source": [
    "1. Classification (BERT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea1ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer for sequence classification\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Input text\n",
    "text = \"I love natural language processing!\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Print the output (predicted label)\n",
    "predicted_label = outputs.logits.argmax().item()\n",
    "print(\"Predicted Label:\", predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f26605",
   "metadata": {},
   "source": [
    "2. Text Generation (GPT-2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86734843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer for text generation\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Input text\n",
    "text = \"Once upon a time, there was a king...\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=50, truncation=True, padding=True)\n",
    "\n",
    "# Perform inference\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ead97",
   "metadata": {},
   "source": [
    "3. Question Answering (BERT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd94e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer for question answering\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Input question and context\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"The capital of France is Paris.\"\n",
    "\n",
    "# Tokenize input question and context\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Print the output (answer)\n",
    "answer_start = torch.argmax(outputs.start_logits)\n",
    "answer_end = torch.argmax(outputs.end_logits)\n",
    "answer = tokenizer.decode(inputs.input_ids[0][answer_start:answer_end+1])\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931d50f",
   "metadata": {},
   "source": [
    "4. Named Entity Recognition (RoBERTa):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained RoBERTa model and tokenizer for named entity recognition\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Input text\n",
    "text = \"Paris is a beautiful city located in France.\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Print the output (named entities)\n",
    "named_entities = tokenizer.decode(inputs.input_ids[0][torch.argmax(outputs.logits)])\n",
    "print(\"Named Entities:\", named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d079933",
   "metadata": {},
   "source": [
    "5. Sentiment Analysis (DistilBERT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ed084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained DistilBERT model and tokenizer for sentiment analysis\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Input text\n",
    "text = \"This movie is fantastic! I loved it.\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Print the output (sentiment)\n",
    "sentiment = \"Positive\" if torch.sigmoid(outputs.logits) >= 0.5 else \"Negative\"\n",
    "print(\"Sentiment:\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd302ab6",
   "metadata": {},
   "source": [
    "<h2>Pre-trained Models</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c91ab",
   "metadata": {},
   "source": [
    "BERT (Text Classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4d95f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(\"This is a sample text for classification\", return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Print classification result\n",
    "predicted_class_index = outputs.logits.argmax().item()\n",
    "predicted_class = model.config.id2label[predicted_class_index]\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe9d03",
   "metadata": {},
   "source": [
    "LLAMA (Data Augmentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afef143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LLAMATokenizer, LLAMAForConditionalGeneration\n",
    "\n",
    "# Load pre-trained LLAMA tokenizer and model\n",
    "tokenizer = LLAMATokenizer.from_pretrained('salesforce/llama-zeroshot')\n",
    "model = LLAMAForConditionalGeneration.from_pretrained('salesforce/llama-zeroshot')\n",
    "\n",
    "# Generate text based on prompt\n",
    "input_text = \"Translate this text into French: Hello, how are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Print augmented text\n",
    "augmented_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Augmented text:\", augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10887fd7",
   "metadata": {},
   "source": [
    "XLNet (Text Generation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7626ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
    "\n",
    "# Load pre-trained XLNet tokenizer and model\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Generate text based on prompt\n",
    "input_text = \"The quick brown fox\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], max_length=50)\n",
    "\n",
    "# Print generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda4298",
   "metadata": {},
   "source": [
    "GPT (Text Completion):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Generate text completion based on prompt\n",
    "input_text = \"Once upon a time, \"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], max_length=100, num_return_sequences=1)\n",
    "\n",
    "# Print text completion\n",
    "completed_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Completed text:\", completed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b8918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
